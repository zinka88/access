{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title: Full Sample Summary\n",
    "# Author: Anna Zink\n",
    "# Date: July 31, 2025\n",
    "# Description: This code pulls in demograhpic information all participants \n",
    "#              merged with the basics survey for information on health insurance, income, employment\n",
    "#              Query code created through the cohort builder option "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD PACKAGES AND FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(bigrquery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "load_data<-function(file, folder){\n",
    "    my_bucket <- Sys.getenv('WORKSPACE_BUCKET')\n",
    "    system(paste0(\"gsutil cp \", my_bucket, folder, file, \" .\"), intern=T)\n",
    "    dsn <- read_csv(file, show_col_types = FALSE)\n",
    "    return(dsn)\n",
    "}\n",
    "\n",
    "# Replace df with THE NAME OF YOUR DATAFRAME\n",
    "# folder = \"/ehr/\" \n",
    "write_csv<-function(df, fn, folder) {\n",
    "   my_dataframe <- df\n",
    "   destination_filename <- fn\n",
    "   write_excel_csv(my_dataframe, destination_filename)\n",
    "   my_bucket <- Sys.getenv('WORKSPACE_BUCKET')\n",
    "   system(paste0(\"gsutil cp ./\", destination_filename, \" \", my_bucket, folder), intern=T)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUERY ALL PARTICIPANTS WITH DEMOGRAPHIC DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This query represents dataset \"ALL_PARTICIPANTS\" for domain \"person\" and was generated for All of Us Controlled Tier Dataset v8\n",
    "dataset_21703205_person_sql <- paste(\"\n",
    "    SELECT\n",
    "        person.person_id,\n",
    "        person.gender_concept_id,\n",
    "        p_gender_concept.concept_name as gender,\n",
    "        person.birth_datetime as date_of_birth,\n",
    "        person.race_concept_id,\n",
    "        p_race_concept.concept_name as race,\n",
    "        person.ethnicity_concept_id,\n",
    "        p_ethnicity_concept.concept_name as ethnicity,\n",
    "        person.sex_at_birth_concept_id,\n",
    "        p_sex_at_birth_concept.concept_name as sex_at_birth,\n",
    "        person.self_reported_category_concept_id,\n",
    "        p_self_reported_category_concept.concept_name as self_reported_category \n",
    "    FROM\n",
    "        `person` person \n",
    "    LEFT JOIN\n",
    "        `concept` p_gender_concept \n",
    "            ON person.gender_concept_id = p_gender_concept.concept_id \n",
    "    LEFT JOIN\n",
    "        `concept` p_race_concept \n",
    "            ON person.race_concept_id = p_race_concept.concept_id \n",
    "    LEFT JOIN\n",
    "        `concept` p_ethnicity_concept \n",
    "            ON person.ethnicity_concept_id = p_ethnicity_concept.concept_id \n",
    "    LEFT JOIN\n",
    "        `concept` p_sex_at_birth_concept \n",
    "            ON person.sex_at_birth_concept_id = p_sex_at_birth_concept.concept_id \n",
    "    LEFT JOIN\n",
    "        `concept` p_self_reported_category_concept \n",
    "            ON person.self_reported_category_concept_id = p_self_reported_category_concept.concept_id\", sep=\"\")\n",
    "\n",
    "# Formulate a Cloud Storage destination path for the data exported from BigQuery.\n",
    "# NOTE: By default data exported multiple times on the same day will overwrite older copies.\n",
    "#       But data exported on a different days will write to a new location so that historical\n",
    "#       copies can be kept as the dataset definition is changed.\n",
    "person_21703205_path <- file.path(\n",
    "  Sys.getenv(\"WORKSPACE_BUCKET\"),\n",
    "  \"bq_exports\",\n",
    "  Sys.getenv(\"OWNER_EMAIL\"),\n",
    "  #strftime(lubridate::now(), \"%Y%m%d\"),  # Comment out this line if you want the export to always overwrite.\n",
    "  \"person_21703205\",\n",
    "  \"person_21703205_*.csv\")\n",
    "message(str_glue('The data will be written to {person_21703205_path}. Use this path when reading ',\n",
    "                 'the data into your notebooks in the future.'))\n",
    "\n",
    "# Perform the query and export the dataset to Cloud Storage as CSV files.\n",
    "# NOTE: You only need to run `bq_table_save` once. After that, you can\n",
    "#       just read data from the CSVs in Cloud Storage.\n",
    "bq_table_save(\n",
    "  bq_dataset_query(Sys.getenv(\"WORKSPACE_CDR\"), dataset_21703205_person_sql, billing = Sys.getenv(\"GOOGLE_PROJECT\")),\n",
    "  person_21703205_path,\n",
    "  destination_format = \"CSV\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data directly from Cloud Storage into memory.\n",
    "# NOTE: Alternatively you can `gsutil -m cp {person_21703205_path}` to copy these files\n",
    "#       to the Jupyter disk.\n",
    "read_bq_export_from_workspace_bucket <- function(export_path) {\n",
    "  col_types <- cols(gender = col_character(), race = col_character(), ethnicity = col_character(), sex_at_birth = col_character(), self_reported_category = col_character())\n",
    "  bind_rows(\n",
    "    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),\n",
    "        function(csv) {\n",
    "          message(str_glue('Loading {csv}.'))\n",
    "          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)\n",
    "          if (is.null(col_types)) {\n",
    "            col_types <- spec(chunk)\n",
    "          }\n",
    "          chunk\n",
    "        }))\n",
    "}\n",
    "person_df <- read_bq_export_from_workspace_bucket(person_21703205_path)\n",
    "\n",
    "dim(person_df)\n",
    "\n",
    "head(person_df, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD BASICS SURVEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basics<-load_data('basics_survey_byperson.csv', \"/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge together, keep everyone with demograhpic information\n",
    "all<-merge(person_df, basics, by='person_id',all.x=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD ACS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This query represents dataset \"ACS\" for domain \"zip_code_socioeconomic\" and was generated for All of Us Controlled Tier Dataset v8\n",
    "dataset_07773787_zip_code_socioeconomic_sql <- paste(\"\n",
    "    SELECT\n",
    "        observation.person_id,\n",
    "        observation.observation_datetime,\n",
    "        zip_code.zip3_as_string as zip_code,\n",
    "        zip_code.fraction_assisted_income as assisted_income,\n",
    "        zip_code.fraction_high_school_edu as high_school_education,\n",
    "        zip_code.median_income,\n",
    "        zip_code.fraction_no_health_ins as no_health_insurance,\n",
    "        zip_code.fraction_poverty as poverty,\n",
    "        zip_code.fraction_vacant_housing as vacant_housing,\n",
    "        zip_code.deprivation_index,\n",
    "        zip_code.acs as american_community_survey_year \n",
    "    FROM\n",
    "        `zip3_ses_map` zip_code \n",
    "    JOIN\n",
    "        `observation` observation \n",
    "            ON CAST(SUBSTR(observation.value_as_string, 0, STRPOS(observation.value_as_string, '*') - 1) AS INT64) = zip_code.zip3 \n",
    "            AND observation_source_concept_id = 1585250 \n",
    "            AND observation.value_as_string NOT LIKE 'Res%'\", sep=\"\")\n",
    "\n",
    "# Formulate a Cloud Storage destination path for the data exported from BigQuery.\n",
    "# NOTE: By default data exported multiple times on the same day will overwrite older copies.\n",
    "#       But data exported on a different days will write to a new location so that historical\n",
    "#       copies can be kept as the dataset definition is changed.\n",
    "zip_code_socioeconomic_07773787_path <- file.path(\n",
    "  Sys.getenv(\"WORKSPACE_BUCKET\"),\n",
    "  \"bq_exports\",\n",
    "  Sys.getenv(\"OWNER_EMAIL\"),\n",
    "  #strftime(lubridate::now(), \"%Y%m%d\"),  # Comment out this line if you want the export to always overwrite.\n",
    "  \"zip_code_socioeconomic_07773787\",\n",
    "  \"zip_code_socioeconomic_07773787_*.csv\")\n",
    "message(str_glue('The data will be written to {zip_code_socioeconomic_07773787_path}. Use this path when reading ',\n",
    "                 'the data into your notebooks in the future.'))\n",
    "\n",
    "# Perform the query and export the dataset to Cloud Storage as CSV files.\n",
    "# NOTE: You only need to run `bq_table_save` once. After that, you can\n",
    "#       just read data from the CSVs in Cloud Storage.\n",
    "#bq_table_save(\n",
    "#  bq_dataset_query(Sys.getenv(\"WORKSPACE_CDR\"), dataset_07773787_zip_code_socioeconomic_sql, billing = Sys.getenv(\"GOOGLE_PROJECT\")),\n",
    "#  zip_code_socioeconomic_07773787_path,\n",
    "#  destination_format = \"CSV\")\n",
    "\n",
    "\n",
    "# Read the data directly from Cloud Storage into memory.\n",
    "# NOTE: Alternatively you can `gsutil -m cp {zip_code_socioeconomic_07773787_path}` to copy these files\n",
    "#       to the Jupyter disk.\n",
    "read_bq_export_from_workspace_bucket <- function(export_path) {\n",
    "  col_types <- cols(zip3_as_string = col_character())\n",
    "  bind_rows(\n",
    "    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),\n",
    "        function(csv) {\n",
    "          message(str_glue('Loading {csv}.'))\n",
    "          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)\n",
    "          if (is.null(col_types)) {\n",
    "            col_types <- spec(chunk)\n",
    "          }\n",
    "          chunk\n",
    "        }))\n",
    "}\n",
    "zip_code_se_df <- read_bq_export_from_workspace_bucket(zip_code_socioeconomic_07773787_path)\n",
    "\n",
    "dim(zip_code_se_df)\n",
    "\n",
    "head(zip_code_se_df, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge together, keep everyone with demograhpic information\n",
    "all<-merge(all, zip_code_se_df, by='person_id',all.x=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_csv(all,'all_participant_demo.csv',\"/data/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.5.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
